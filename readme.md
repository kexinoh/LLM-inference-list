| 名称 | 描述 | 星标数量 |
|:-----|:------------|------:|
| [llama.cpp](https://github.com/ggerganov/llama.cpp) | 以 C/C++ 实现的 LLM 推理框架，适合资源受限设备 | 79,582 |
| [vllm](https://github.com/vllm-project/vllm) | 高吞吐量和内存高效的 LLM 推理和服务的引擎 | 47,035 |
| [web-llm](https://github.com/mlc-ai/web-llm) | 高性能的浏览器内 LLM 推理引擎 | 15,422 |
| [SGLang](https://github.com/sgl-project/sglang) | 支持大型语言和视觉语言模型的快速服务框架 | 13,900 |
| [OpenLLM](https://github.com/bentoml/OpenLLM) | 支持运行任何开源 LLM（如 DeepSeek、Llama），提供 OpenAI 兼容的云端 API 端点 | 11,200 |
| [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) | 为 NVIDIA GPU 优化的 LLM 推理框架，提供易用的 Python API 和先进优化 | 10,456 |
| [text-generation-inference](https://github.com/huggingface/text-generation-inference) | 大型语言模型的文本生成推理工具 | 10,102 |
| [FlexGen](https://github.com/FMInference/FlexGen) | 在单 GPU 上运行大型语言模型，面向吞吐量优化场景 | 9,316 |
| [Triton Inference Server](https://github.com/triton-inference-server/server) | 提供优化的云端和边缘推理解决方案 | 9,187 |
| [lmdeploy](https://github.com/InternLM/lmdeploy) | 用于压缩、部署和服务 LLM 的工具包 | 6,322 |
| [exllamav2](https://github.com/turboderp/exllamav2) | 在现代消费级 GPU 上本地运行 LLM 的快速推理库 | 4,167 |
| [exllama](https://github.com/turboderp/exllama) | 针对量化权重的 Llama 模型实现，内存效率更高 | 2,871 |
| [DeepSpeed-MII](https://github.com/deepspeedai/DeepSpeed-MII) | 由 DeepSpeed 提供支持，实现低延迟和高吞吐量的推理 | 2,000 |
